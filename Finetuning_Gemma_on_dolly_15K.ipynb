{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/SwayamInSync/prompt-expansion/blob/main/Finetuning_Gemma_on_dolly_15K.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0_Zt6PnpsTOp"
      },
      "source": [
        "## 1. Setup development environment\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nTsOGysxsTOp"
      },
      "outputs": [],
      "source": [
        "# Install Pytorch & other libraries\n",
        "!pip install \"torch==2.1.2\" tensorboard\n",
        "\n",
        "# Install Hugging Face libraries\n",
        "!pip install  --upgrade \\\n",
        "  \"transformers==4.38.2\" \\\n",
        "  \"datasets==2.16.1\" \\\n",
        "  \"accelerate==0.26.1\" \\\n",
        "  \"evaluate==0.4.1\" \\\n",
        "  \"bitsandbytes==0.42.0\" \\\n",
        "  \"trl==0.7.11\" \\\n",
        "  \"peft==0.8.2\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bYjO4yESsTOr"
      },
      "outputs": [],
      "source": [
        "import torch; assert torch.cuda.get_device_capability()[0] >= 8, 'Hardware not supported for Flash Attention'\n",
        "# install flash-attn\n",
        "!pip install ninja packaging\n",
        "!MAX_JOBS=4 pip install flash-attn --no-build-isolation --upgrade"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cG-uA61ssTOs"
      },
      "outputs": [],
      "source": [
        "from huggingface_hub import notebook_login\n",
        "\n",
        "notebook_login()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CYwW9imFsTOs"
      },
      "source": [
        "## 2. Create and prepare the dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NUrFK95qsTOt"
      },
      "outputs": [],
      "source": [
        "from datasets import load_dataset\n",
        "\n",
        "# Load Dolly Dataset.\n",
        "dataset = load_dataset(\"philschmid/dolly-15k-oai-style\", split=\"train\")\n",
        "\n",
        "print(dataset[3][\"messages\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_0hZpDbLsTOt"
      },
      "source": [
        "## 3. Fine-tune LLM using `trl` and the `SFTTrainer`\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VlakAoqQsTOt"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
        "\n",
        "# Hugging Face model id\n",
        "model_id = \"google/gemma-7b\"\n",
        "tokenizer_id = \"philschmid/gemma-tokenizer-chatml\"\n",
        "\n",
        "# BitsAndBytesConfig int-4 config\n",
        "bnb_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True, bnb_4bit_use_double_quant=True, bnb_4bit_quant_type=\"nf4\", bnb_4bit_compute_dtype=torch.bfloat16\n",
        ")\n",
        "\n",
        "# Load model and tokenizer\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_id,\n",
        "    device_map=\"auto\",\n",
        "    attn_implementation=\"flash_attention_2\",\n",
        "    torch_dtype=torch.bfloat16,\n",
        "    quantization_config=bnb_config\n",
        ")\n",
        "tokenizer = AutoTokenizer.from_pretrained(tokenizer_id)\n",
        "tokenizer.padding_side = 'right' # to prevent warnings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rU3MMuAjsTOu"
      },
      "outputs": [],
      "source": [
        "from peft import LoraConfig\n",
        "\n",
        "# LoRA config based on QLoRA paper & Sebastian Raschka experiment\n",
        "peft_config = LoraConfig(\n",
        "        lora_alpha=8,\n",
        "        lora_dropout=0.05,\n",
        "        r=6,\n",
        "        bias=\"none\",\n",
        "        target_modules=\"all-linear\",\n",
        "        task_type=\"CAUSAL_LM\",\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LEUeqLABsTOu"
      },
      "outputs": [],
      "source": [
        "from transformers import TrainingArguments\n",
        "\n",
        "args = TrainingArguments(\n",
        "    output_dir=\"gemma-7b-dolly-chatml\", # directory to save and repository id\n",
        "    num_train_epochs=3,                     # number of training epochs\n",
        "    per_device_train_batch_size=2,          # batch size per device during training\n",
        "    gradient_accumulation_steps=2,          # number of steps before performing a backward/update pass\n",
        "    gradient_checkpointing=True,            # use gradient checkpointing to save memory\n",
        "    optim=\"adamw_torch_fused\",              # use fused adamw optimizer\n",
        "    logging_steps=10,                       # log every 10 steps\n",
        "    save_strategy=\"epoch\",                  # save checkpoint every epoch\n",
        "    bf16=True,                              # use bfloat16 precision\n",
        "    tf32=True,                              # use tf32 precision\n",
        "    learning_rate=2e-4,                     # learning rate, based on QLoRA paper\n",
        "    max_grad_norm=0.3,                      # max gradient norm based on QLoRA paper\n",
        "    warmup_ratio=0.03,                      # warmup ratio based on QLoRA paper\n",
        "    lr_scheduler_type=\"constant\",           # use constant learning rate scheduler\n",
        "    push_to_hub=False,                       # push model to hub\n",
        "    report_to=\"tensorboard\",                # report metrics to tensorboard\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-5kYSui1sTOu"
      },
      "outputs": [],
      "source": [
        "from trl import SFTTrainer\n",
        "\n",
        "max_seq_length = 1512 # max sequence length for model and packing of the dataset\n",
        "\n",
        "trainer = SFTTrainer(\n",
        "    model=model,\n",
        "    args=args,\n",
        "    train_dataset=dataset,\n",
        "    peft_config=peft_config,\n",
        "    max_seq_length=max_seq_length,\n",
        "    tokenizer=tokenizer,\n",
        "    packing=True,\n",
        "    dataset_kwargs={\n",
        "        \"add_special_tokens\": False, # We template with special tokens\n",
        "        \"append_concat_token\": False, # No need to add additional separator token\n",
        "    }\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c2AMvZgQsTOu"
      },
      "outputs": [],
      "source": [
        "# start training, the model will be automatically saved to the hub and the output directory\n",
        "trainer.train()\n",
        "\n",
        "# save model\n",
        "trainer.save_model()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HbzzA8dvsTOu"
      },
      "source": [
        "## 3. Test Model and run Inference"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f3V2zYetsTOv"
      },
      "outputs": [],
      "source": [
        "# free the memory again\n",
        "del model\n",
        "del trainer\n",
        "torch.cuda.empty_cache()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zvnk4h1VsTOv"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from peft import AutoPeftModelForCausalLM\n",
        "from transformers import  AutoTokenizer, pipeline\n",
        "\n",
        "peft_model_id = \"gemma-7b-dolly-chatml\"\n",
        "\n",
        "# Load Model with PEFT adapter\n",
        "tokenizer = AutoTokenizer.from_pretrained(peft_model_id)\n",
        "model = AutoPeftModelForCausalLM.from_pretrained(peft_model_id, device_map=\"auto\", torch_dtype=torch.float16)\n",
        "pipe = pipeline(\"text-generation\", model=model, tokenizer=tokenizer)\n",
        "# get token id for end of conversation\n",
        "eos_token = tokenizer(\"<|im_end|>\",add_special_tokens=False)[\"input_ids\"][0]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dcM6hReasTOv"
      },
      "source": [
        "Lets test some prompt samples and see how the model performs."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jXabVkSusTOv",
        "outputId": "5c125350-9fe5-4a6c-a490-f89b99d5d3c8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "    prompt:\n",
            "What is the capital of Germany? Explain why thats the case and if it was different in the past?\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/opt/conda/envs/pytorch/lib/python3.10/site-packages/transformers/pipelines/base.py:1157: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "    response:\n",
            "The capital of Germany is Berlin. Berlin is the capital of Germany because it is the largest city in Germany, and it is the political center of the country. Berlin is also home to the German parliament, the Bundestag, and the federal government. In the past, the capital of Germany was Berlin, but it was divided into two parts during the Cold War. The western part of Berlin was controlled by the United States, France, and the United Kingdom, while the eastern part was controlled by the Soviet Union. Berlin was also the site of the Berlin Wall, which separated the two sides of the city from 1961 to 1989. The fall of the Berlin Wall marked the end of the Cold War, and Germany was reunified in 1990. Berlin has since become the capital of a reunified Germany.\n",
            "--------------------------------------------------\n",
            "    prompt:\n",
            "Write a Python function to calculate the factorial of a number.\n",
            "    response:\n",
            "def factorial(n):\n",
            "    fact = 1\n",
            "    for i in range(1, n+1):\n",
            "        fact = fact * i\n",
            "    return fact\n",
            "--------------------------------------------------\n",
            "    prompt:\n",
            "A rectangular garden has a length of 25 feet and a width of 15 feet. If you want to build a fence around the entire garden, how many feet of fencing will you need?\n",
            "    response:\n",
            "The total amount of fencing needed is 70 feet. This is determined by multiplying the perimeter of the garden (which is 70 feet) by the cost per foot of the fencing material.\n",
            "--------------------------------------------------\n",
            "    prompt:\n",
            "What is the difference between a fruit and a vegetable? Give examples of each.\n",
            "    response:\n",
            "A fruit is the part of a plant that contains the seeds. Examples of fruits are apples, oranges, strawberries, and grapes. A vegetable is the part of a plant that is used for food but does not contain seeds. Examples of vegetables are carrots, potatoes, broccoli, and lettuce.\n",
            "--------------------------------------------------\n"
          ]
        }
      ],
      "source": [
        "prompts = [\n",
        "    \"What is the capital of Germany? Explain why thats the case and if it was different in the past?\",\n",
        "    \"Write a Python function to calculate the factorial of a number.\",\n",
        "    \"A rectangular garden has a length of 25 feet and a width of 15 feet. If you want to build a fence around the entire garden, how many feet of fencing will you need?\",\n",
        "    \"What is the difference between a fruit and a vegetable? Give examples of each.\",\n",
        "]\n",
        "\n",
        "def test_inference(prompt):\n",
        "    prompt = pipe.tokenizer.apply_chat_template([{\"role\": \"user\", \"content\": prompt}], tokenize=False, add_generation_prompt=True)\n",
        "    outputs = pipe(prompt, max_new_tokens=1024, do_sample=True, temperature=0.7, top_k=50, top_p=0.95, eos_token_id=eos_token)\n",
        "    return outputs[0]['generated_text'][len(prompt):].strip()\n",
        "\n",
        "\n",
        "for prompt in prompts:\n",
        "    print(f\"    prompt:\\n{prompt}\")\n",
        "    print(f\"    response:\\n{test_inference(prompt)}\")\n",
        "    print(\"-\"*50)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "pytorch",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.13"
    },
    "orig_nbformat": 4,
    "vscode": {
      "interpreter": {
        "hash": "2d58e898dde0263bc564c6968b04150abacfd33eed9b19aaa8e45c040360e146"
      }
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}